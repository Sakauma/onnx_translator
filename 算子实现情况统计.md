### 1. 基础运算和操作
四则运算：
Add (加法)
Sub (减法)
Mul (乘法)
Div (除法)

初等数学函数：
Abs (绝对值)
Cos (余弦函数)
Exp (指数函数 $e^x$) 
Log (自然对数 $\ln x$) 
Sqrt (平方根 $\sqrt{x}$) 
Pow (幂运算 $x^y$) 
Neg (取负)
Reciprocal (倒数)
Clip (截断 $y = min(max(x, min), max) $) 
Ceil (向上取整)
Floor  (向下取整)

比较与极值：
Max (最大值) 
Min (最小值)

### 2. 基础非线性函数和线性代数
非线性激活函数：
Relu ：最经典的激活函数
Sigmoid ($\frac{1}{1+e^{-x}}$)：门控机制核心
Tanh ($\frac{e^x - e^{-x}}{e^x + e^{-x}}$)：双曲正切

线性代数运算：
Gemm (通用矩阵乘法)：全连接层（Linear/Dense）的核心实现，支持 alpha * A * B + beta * C
MatMul (多维矩阵乘法)：混合精度常用

### 3. 神经网络核心组件
特征提取：
Conv (2D 卷积)：支持 Padding, Stride, Dilation 和 Group

降采样：
MaxPool (最大池化)：提取局部最显著特征

概率输出：
Softmax：多分类归一化，通常作为网络的最后一层或 Attention 的一部分

量化工具 (用于模型压缩与加速)：
QuantizeLinear：线性量化（Float -> Int）
DequantizeLinear：反量化（Int -> Float）

### 4. 成熟网络结构与数据操作
形状变换：
Flatten (展平)：常用于卷积层到全连接层的过渡
Reshape (重塑)：万能的形状改变工具

维度操作：
Transpose (转置/维度置换)：如 NCHW <-> NHWC，或 Attention 中的 (B, S, H, D) 变换
Squeeze (压缩)：移除维度为 1 的轴
Unsqueeze (扩展)：在指定位置增加维度为 1 的轴

复杂数据操作：
cast (类型转换)
Concat (拼接)
Slice (切片)

### 5. 注意事项
部分算子不涉及数值计算的，没有验证精度；大部分算子没有验证图
待实现的紧急算子：BatchNormalization、LayerNormalization、GlobalAveragePool、ReduceMean、AveragePool、Gather

懒得分类了，在这里记录每一次commit实现的算子
Constant Shape Gather Expand
加了17个简单的，具体的不列了
加了ConstantOfShape Range Tile Pad Split
加了ReduceMean ReduceSum ReduceMax ReduceMin ReduceProd ArgMax ArgMin
加了 TopK CumSum RandomUniformLike Einsum Upsample
现在覆盖了全部模型解析出的算子，接下来要实现的大部分是为了凑齐70%的要求。

实现第一批凑数的
Elu, Selu, LeakyRelu, ThresholdedRelu, HardSigmoid, Softplus, Softsign, Celu, HardSwish, Shrink
实现第二批凑数的
Acos, Asin, Cosh, Sinh, Asinh, Acosh, Atanh，别问我为什么不用查表，说了不管效率只看正确
实现第三批凑数的
BitwiseAnd, BitwiseOr, BitwiseXor, BitwiseNot, BitShift
实现第四批凑数的
ReduceL1, ReduceL2, ReduceLogSum, ReduceLogSumExp, ReduceSumSquare
实现第五批凑数的
AveragePool, GlobalAveragePool, GlobalMaxPool, LpPool, Mean
实现第六批凑数的
Size, OneHot, Tril, Triu, IsInf

添加五个常见算子
BatchNormalization LayerNormalization InstanceNormalization Round Erf